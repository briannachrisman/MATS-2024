{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-05-24 21:07:44.549956: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-24 21:07:44.592156: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#%pip install einops transformers sae_lens sae_vis\n",
    "import einops\n",
    "import os\n",
    "from utils_folder import dashboards\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes, convert_connor_rob_sae_to_our_saelens_format, download_sae_from_hf\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch as t\n",
    "import plotly_express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n",
    "from torch.utils.data import Dataset\n",
    "from sae_vis.model_fns import AutoEncoderConfig, AutoEncoder\n",
    "from transformer_lens import utils as tl_utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")\n",
    "\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.utils import (\n",
    "    load_dataset,\n",
    "    tokenize_and_concatenate,\n",
    "    download_file_from_hf,\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "from transformer_lens import HookedSAE, HookedSAEConfig\n",
    "from transformer_lens.utils import download_file_from_hf\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "from utils_folder.autoencoder import AutoEncoder\n",
    "import matplotlib as mpl\n",
    "cmap = mpl.colormaps[\"Spectral\"]\n",
    "\n",
    "from utils_folder import SAE_metrics\n",
    "from utils_folder import vector_metrics\n",
    "from utils_folder import misc_utils\n",
    "\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "gelu2 = HookedTransformer.from_pretrained(\"gelu-2l\").to(device)\n",
    "gpt_small = HookedTransformer.from_pretrained(\"gpt2-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = gpt_small.run_with_cache('today I went to the beach and I saw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 10, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_patterns = [cache[k] for k in cache if 'attn.hook_pattern' in k] # layer head query key\n",
    "attn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7586e-01, 6.1720e-02, 9.7753e-02, 1.0556e-01, 1.2122e-01, 1.0165e-01,\n",
       "         4.5974e-02, 1.4088e-01, 1.4243e-01, 6.9495e-03],\n",
       "        [4.6450e-01, 5.6798e-02, 6.1003e-02, 4.7216e-02, 7.8358e-02, 8.2871e-02,\n",
       "         4.2495e-02, 1.2238e-01, 4.1550e-02, 2.8317e-03],\n",
       "        [4.5900e-01, 2.0960e-02, 9.2207e-02, 7.3440e-02, 1.1960e-01, 7.3603e-02,\n",
       "         3.3193e-02, 8.0797e-02, 3.0337e-02, 1.6855e-02],\n",
       "        [7.3982e-01, 5.6579e-02, 4.7204e-02, 4.3899e-02, 3.1680e-02, 2.4851e-02,\n",
       "         2.1732e-02, 1.7152e-02, 1.0241e-02, 6.8400e-03],\n",
       "        [7.2622e-01, 5.5624e-02, 3.4963e-02, 4.7602e-02, 3.1507e-02, 2.7072e-02,\n",
       "         3.3840e-02, 1.7616e-02, 1.1864e-02, 1.3690e-02],\n",
       "        [7.7203e-01, 3.4316e-02, 5.9629e-02, 1.9105e-02, 4.4625e-02, 2.0528e-02,\n",
       "         4.0044e-03, 3.0037e-02, 9.6162e-03, 6.1102e-03],\n",
       "        [9.0993e-01, 2.3547e-02, 2.1805e-02, 1.2051e-02, 7.0969e-03, 8.8852e-03,\n",
       "         7.3795e-03, 2.9199e-03, 2.2177e-03, 4.1655e-03],\n",
       "        [8.2964e-01, 4.3540e-02, 3.8828e-02, 2.5020e-02, 2.0136e-02, 1.4513e-02,\n",
       "         1.5414e-02, 7.6664e-03, 4.8864e-03, 3.5655e-04],\n",
       "        [8.1983e-01, 3.4837e-02, 2.9336e-02, 2.3989e-02, 3.5325e-02, 3.4491e-02,\n",
       "         5.5496e-03, 1.0430e-02, 4.3259e-03, 1.8913e-03],\n",
       "        [8.6258e-01, 2.9692e-02, 2.3456e-02, 1.2232e-02, 1.9049e-02, 3.7554e-02,\n",
       "         2.4604e-03, 7.1551e-03, 4.3424e-03, 1.4756e-03],\n",
       "        [2.8192e-03, 2.5190e-01, 1.7491e-01, 1.7427e-01, 1.0462e-01, 7.8010e-02,\n",
       "         8.9028e-02, 5.2418e-02, 3.6948e-02, 3.5070e-02],\n",
       "        [1.3730e-01, 1.0537e-02, 7.2434e-03, 6.7856e-03, 5.3059e-02, 8.5623e-03,\n",
       "         4.6579e-03, 4.0166e-02, 1.5384e-03, 7.3015e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(len(attn_patterns)):\n",
    "    attn_pattern = attn_patterns[0][0,:,:,:]\n",
    "    contribution = attn_pattern[:,-1,:]\n",
    "    contribution = contribution.mean(dim=0)\n",
    "\n",
    "    cumm_attn = \n",
    "    for prev_layer in range(layer-1):\n",
    "        cumm_attn = \n",
    "\n",
    "\n",
    "    contribution_so_far = contribution \n",
    "    \n",
    "\n",
    "attn_pattern1 = attn_patterns[1][0,:,:,:]\n",
    "contribution1 = attn_pattern1[:,-1,:]\n",
    "contribution1 = einops.einsum(contribution1, attn_pattern1, 'n_heads new_token, n_heads new_token old_token -> n_heads old_token')\n",
    "contribution1 = contribution1.mean(dim=0)\n",
    "\n",
    "contribution_so_far = contribution_so_far + contribution1\n",
    "\n",
    "\n",
    "attn_pattern2 = attn_patterns[2][0,:,:,:]\n",
    "contribution2 = attn_pattern2[:,-1,:]\n",
    "contribution2 = einops.einsum(contribution2, attn_pattern2, 'n_heads new_token, n_heads new_token old_token -> n_heads old_token')\n",
    "contribution2 = einops.einsum(contribution2, attn_pattern1, 'n_heads new_token, n_heads new_token old_token -> n_heads old_token')\n",
    "contribution2 = contribution2.mean(dim=0)\n",
    "\n",
    "\n",
    "contribution_so_far = contribution_so_far + contribution2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cont = contribution_so_far/contribution_so_far.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cont = contribution_so_far/contribution_so_far.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3822, 0.0595, 0.0514, 0.0526, 0.0526, 0.0444, 0.0344, 0.0563, 0.0469,\n",
       "        0.2197], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_cont"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
