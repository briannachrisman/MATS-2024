{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import gc\n",
    "from utils import load_our_sae, get_tokens\n",
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "hook_point_layer=1\n",
    "hook_point=f\"blocks.{hook_point_layer}.attn.hook_z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE for head # 6\n",
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391e6a289c3345dbbbcd00e45ffbb881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aaaf9fe2d354b5c87fc317c05b266ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:26<00:00, 38.42it/s]\n"
     ]
    }
   ],
   "source": [
    "hook_point_head_index = 6\n",
    "model, our_sae, activations_loader = load_our_sae(hook_point_head_index=hook_point_head_index)\n",
    "all_tokens = get_tokens(activations_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535eaad5b665453b8c9f771a0a99ddaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to cache data for vis:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fb7e803de640b1a781faafe948f375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vis data from cached data:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num chunks: 1, chunk size: 1000\n",
      "torch.Size([1, 2048]) torch.Size([2350])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time  </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.03s │ 3.2%  │\n",
       "│ (2) Forward passes to gather model activations │ 0.78s │ 75.0% │\n",
       "│ (3) Computing feature acts from model acts     │ 0.01s │ 0.8%  │\n",
       "│ (4) Getting data for tables                    │ 0.02s │ 1.7%  │\n",
       "│ (5) Getting data for histograms                │ 0.02s │ 1.5%  │\n",
       "│ (6) Getting data for sequences                 │ 0.17s │ 16.2% │\n",
       "│ (7) Getting data for quantiles                 │ 0.02s │ 1.6%  │\n",
       "└────────────────────────────────────────────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.03s │ 3.2%  │\n",
       "│ (2) Forward passes to gather model activations │ 0.78s │ 75.0% │\n",
       "│ (3) Computing feature acts from model acts     │ 0.01s │ 0.8%  │\n",
       "│ (4) Getting data for tables                    │ 0.02s │ 1.7%  │\n",
       "│ (5) Getting data for histograms                │ 0.02s │ 1.5%  │\n",
       "│ (6) Getting data for sequences                 │ 0.17s │ 16.2% │\n",
       "│ (7) Getting data for quantiles                 │ 0.02s │ 1.6%  │\n",
       "└────────────────────────────────────────────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff45543e8794164bd4ac939cec4ee45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gc.collect(); torch.cuda.empty_cache()\n",
    "# WARNING: ALWAYS MAKE SURE TO CHECK hook_point_head_index in the cell above. \n",
    "test_feature_idx_gpt = list(range(3, 4))\n",
    "\n",
    "# setting batch_size = 8192 and minibatch_size_tokens = 32 avoids OOMS with A100 GPUs.\n",
    "feature_vis_config_gpt = SaeVisConfig(\n",
    "    hook_point=hook_point,\n",
    "    hook_point_head_index=hook_point_head_index,\n",
    "    features=test_feature_idx_gpt,\n",
    "    batch_size=2, \n",
    "    minibatch_size_tokens=16,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "sae_vis_data_gpt = SaeVisData.create(\n",
    "    encoder=our_sae,\n",
    "    model=model, # type: ignore\n",
    "    tokens=all_tokens,  # type: ignore\n",
    "    cfg=feature_vis_config_gpt,\n",
    ")\n",
    "\n",
    "gc.collect(); torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "output_dir = f\"./dashboards/head{hook_point_head_index}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for feature in test_feature_idx_gpt:\n",
    "    filename = os.path.join(output_dir, f\"{feature}.html\")\n",
    "    sae_vis_data_gpt.save_feature_centric_vis(filename, feature)\n",
    "    # display_vis_inline(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
